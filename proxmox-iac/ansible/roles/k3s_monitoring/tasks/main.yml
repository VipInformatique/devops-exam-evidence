---
# ansible/roles/k3s_monitoring/tasks/main.yml

- name: Ensure monitoring namespace exists
  kubernetes.core.k8s:
    name: monitoring
    api_version: v1
    kind: Namespace
    state: present
    kubeconfig: "{{ k3s_kubeconfig }}"

- name: Zainstaluj Kube Prometheus Stack (kompletna konfiguracja)
  kubernetes.core.helm:
    name: kube-prometheus-stack
    chart_ref: prometheus-community/kube-prometheus-stack
    release_namespace: monitoring
    create_namespace: false
    chart_version: "75.15.0"
    kubeconfig: "{{ k3s_kubeconfig }}"
    values:
      prometheus:
        prometheusSpec:
          resources:
            requests:
              cpu: 500m
              memory: 1Gi
            limits:
              cpu: "2"
              memory: 4Gi
          storageSpec:
            volumeClaimTemplate:
              spec:
                storageClassName: nfs-client
                resources:
                  requests:
                    storage: 10Gi
          nodeSelector:
            role: worker
          additionalScrapeConfigs:
            - job_name: "node-exporter"
              static_configs:
                - targets: ["192.168.100.2:9100", "192.168.100.3:9100", "192.168.100.4:9100"]
        service:
          type: LoadBalancer
          loadBalancerIP: 192.168.88.14  # <-- Tylko ten serwis "rezerwuje" statyczny IP
          loadBalancerSourceRanges:
           - 10.10.10.0/24
          annotations:
            metallb.universe.tf/allow-shared-ip: monitoring # Klucz współdzielenia
          port: 9090

      grafana:
        nodeSelector:
          role: worker
        adminPassword: "{{ grafana_admin_password }}"
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
        persistence:
          enabled: true
          type: pvc
          storageClassName: nfs-client
          accessModes: [ReadWriteOnce]
          size: 2Gi
        sidecar:
          dashboards:
            enabled: true
            searchNamespace: ALL
          datasources:
            enabled: true
            defaultDatasourceEnabled: false
            additionalDataSources:
              - name: Prometheus
                type: prometheus
                url: http://kube-prometheus-stack-prometheus:9090
                access: proxy
                isDefault: true
                uid: prometheus
                jsonData:
                  httpMethod: POST
                  manageAlerts: true
                  prometheusType: Prometheus
                  prometheusVersion: "2.45.0"
              - name: Loki
                type: loki
                url: http://loki:3100
                access: proxy
                uid: loki
                isDefault: false
                jsonData:
                  maxLines: "1000"
        service:
          type: LoadBalancer
          # loadBalancerIP usunięty - dołączy do IP Prometheusa
          loadBalancerSourceRanges:
           - 10.10.10.0/24
          annotations:
            metallb.universe.tf/allow-shared-ip: monitoring # Ten sam klucz
          port: 3000

      prometheus-node-exporter:
        tolerations:
          - key: "node-role.kubernetes.io/control-plane"
            operator: "Exists"

      alertmanager:
        configSecret: "{{ alertmanager_secret_name }}"  
        alertmanagerSpec:
          nodeSelector:
            role: worker
          containers:
          - name: alertmanager
            env:
            - name: TZ
              value: "Europe/Warsaw"

      kube-state-metrics:
        nodeSelector:
          role: worker

      prometheusOperator:
        nodeSelector:
          role: worker
          
      # Konfiguracja wyłączająca zbędne komponenty (przeniesiona z usuniętego taska)
      kubeControllerManager:
        enabled: false
      kubeScheduler:
        enabled: false
      kubeProxy:
        enabled: false
      kubeEtcd:
        enabled: false
      defaultRules:
        rules:
          kubeControllerManager: false
          kubeScheduler: false
          kubeProxy: false
          etcd: false

    timeout: "15m0s"
    wait: true

- name: Zainstaluj Loki
  kubernetes.core.helm:
    name: loki
    chart_ref: grafana/loki
    release_namespace: monitoring
    create_namespace: false
    kubeconfig: "{{ k3s_kubeconfig }}"
    values:
      lokiCanary:
        enabled: false
      test:
        enabled: false
      deploymentMode: SingleBinary
      monitoring:
        serviceMonitor:
          enabled: true
          labels:
            release: kube-prometheus-stack
      read:
        replicas: 0
      write:
        replicas: 0
      backend:
        replicas: 0
      gateway:
        enabled: false
      memberlist:
        enabled: false
      chunksCache:
        enabled: false
      resultsCache:
        enabled: false
      loki:
        auth_enabled: false
        storage:
          type: filesystem
        commonConfig:
          replication_factor: 1
        limits_config:
          retention_period: 7d
          volume_enabled: true
        schemaConfig:
          configs:
            - from: "2023-01-01"  # ZMIENIONO z 2025 na 2023
              store: tsdb
              object_store: filesystem
              schema: v13
              index:
                prefix: loki_index_  # ZMIENIONO z index_ na loki_index_
                period: 24h
      persistence:
        enabled: true
        storageClassName: nfs-client
        size: 10Gi
      singleBinary:
        nodeSelector:
          role: worker
        resources:
          requests:
            cpu: 200m
            memory: 512Mi
          limits:
            cpu: "1"
            memory: 2Gi
        service:
          type: LoadBalancer
          loadBalancerIP: 192.168.88.14
          loadBalancerSourceRanges:
           - 10.10.10.0/24                       # ogranicz do VPN
          annotations:
            metallb.universe.tf/allow-shared-ip: monitoring
          port: 3100
    timeout: "5m0s"
    wait: true


- name: Zainstaluj Promtail (wszystko inline - zgodnie z twoimi values)
  kubernetes.core.helm:
    name: promtail
    chart_ref: grafana/promtail
    release_namespace: monitoring
    create_namespace: false
    kubeconfig: "{{ k3s_kubeconfig }}"
    values:
      resources:
        requests: { cpu: 100m, memory: 128Mi }
        limits:   { cpu: 300m, memory: 256Mi }
      config:
        logLevel: warn           # było: info — mniej hałasu własnych logów Promtail
        clients:
          - url: http://loki.monitoring.svc:3100/loki/api/v1/push
        snippets:
          # === DODANE: twarde dropy zanim cokolwiek zrobimy dalej ===
          extraRelabelConfigs:
            # 0) NIE tailuj zakończonych podów (główne źródło "file does not exist")
            - action: drop
              source_labels: ['__meta_kubernetes_pod_phase']
              regex: 'Succeeded|Failed'

            # 0b) (opcjonalnie) wytnij cały namespace sqldump
            - action: drop
              source_labels: ['__meta_kubernetes_namespace']
              regex: 'sqldump'

            # 0c) (opcjonalnie) wytnij job cleanup
            - action: drop
              source_labels: ['__meta_kubernetes_pod_label_job_name','__meta_kubernetes_pod_label_job-name']
              regex: 'neon-marker-cleanup'

            # === Twoje istniejące reguły pozostają ===
            # Czytelne labele z poda
            - action: replace
              source_labels: ['__meta_kubernetes_pod_label_app']
              target_label: app
              regex: '(.+)'
              replacement: '$1'
            - action: replace
              source_labels: ['__meta_kubernetes_pod_label_app_kubernetes_io_name']
              target_label: app_kubernetes_io_name
              regex: '(.+)'
              replacement: '$1'
            - action: replace
              source_labels: ['__meta_kubernetes_pod_label_app_kubernetes_io_component']
              target_label: component
              regex: '(.+)'
              replacement: '$1'

            # Fallback dla "service"
            - action: replace
              source_labels: ['service','app']
              target_label: service
              regex: '^$;(.+)'
              replacement: '$1'
            - action: replace
              source_labels: ['service','app_kubernetes_io_name']
              target_label: service
              regex: '^$;(.+)'
              replacement: '$1'
            - action: replace
              source_labels: ['service','component']
              target_label: service
              regex: '^$;(.+)'
              replacement: '$1'
            - action: replace
              source_labels: ['service','__meta_kubernetes_pod_name']
              target_label: service
              regex: '^$;(.+)'
              replacement: '$1'
            - action: replace
              source_labels: ['service','service_name']
              target_label: service
              regex: '^$;(.+)'
              replacement: '$1'

          pipelineStages:
            # 1) Globalnie: utnij linie będące samym "pppp..." lub "NUMER ppppp..."
            - match:
                selector: '{namespace=~".+"}'
                stages:
                  - drop:
                      expression: '^([0-9]{6,}\s+)?[pP]{8,}\s*$'

            # 2) Szum z samego Lokiego
            - match:
                selector: '{namespace="monitoring", pod=~"loki-.*", container="loki"}'
                stages:
                  - drop:
                      expression: 'executing query|starting to tail logs|ended tailing logs|roundtrip\.go:|engine\.go:|metrics\.go:'

            # 3) Szum z wtyczki Loki w Grafanie (opcjonalnie)
            - match:
                selector: '{namespace="monitoring", app_kubernetes_io_name="grafana"}'
                stages:
                  - drop:
                      expression: 'logger=tsdb\.loki|pluginId=loki|endpoint=queryData'
    timeout: "5m0s"
    wait: true


- name: Sprawdź status podów monitoringu
  ansible.builtin.command: kubectl get pods -n monitoring
  environment:
    KUBECONFIG: "{{ k3s_kubeconfig }}"
  register: monitoring_pods
  changed_when: false

- name: Sprawdź status serwisów monitoringu
  ansible.builtin.command: kubectl get svc -n monitoring
  environment:
    KUBECONFIG: "{{ k3s_kubeconfig }}"
  register: monitoring_services
  changed_when: false

- name: Podsumowanie deploymentu
  ansible.builtin.debug:
    msg:
      - "=== STATUS PODÓW ==="
      - "{{ monitoring_pods.stdout_lines }}"
      - "=== STATUS SERWISÓW ==="
      - "{{ monitoring_services.stdout_lines }}"
      - "=== DOSTĘP ==="
      - "Grafana: http://192.168.88.14:3000 (admin/{{ grafana_admin_password }})"
      - "Prometheus: http://192.168.88.14:9090"
      - "Loki: http://192.168.88.14:3100"