apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: devistor-alerts
  namespace: {{ monitoring_ns }}
  labels:
    managed-by: devistor
    release: {{ kps_release }}
spec:
  groups:

  # -------------------------------
  # Critical alerts related to Kubernetes nodes
  # -------------------------------
  - name: k8s-critical
    rules:
    - alert: HighNodeCPU
      expr: avg by (instance) (rate(node_cpu_seconds_total{mode!="idle"}[5m])) > 0.9
      for: 5m
      labels: { severity: "critical" }
      annotations:
        summary: "{% raw %}CPU > 90% sur {{ $labels.instance }}{% endraw %}"
        description: "Sustained high CPU load detected"

  # -------------------------------
  # Warning alerts related to Kubernetes nodes
  # -------------------------------
  - name: k8s-warning
    rules:
    - alert: PodCrashLoopBackOff
      expr: max by (namespace, pod, container) (kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff"}) == 1
      for: 5m
      labels: { severity: "warning" }
      annotations:
        summary: "{% raw %}CrashLoopBackOff: {{ $labels.namespace }}/{{ $labels.pod }}{% endraw %}"
        description: "{% raw %}Container {{ $labels.container }} has been restarting for ≥5m.{% endraw %}"

    - alert: NodeDiskPressure
      expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 5m
      labels: { severity: "warning" }
      annotations:
        summary: "{% raw %}Node under disk pressure: {{ $labels.node }}{% endraw %}"
        description: "Check free space / I/O load"

    - alert: NodeMemoryPressure
      expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
      for: 5m
      labels: { severity: "warning" }
      annotations:
        summary: "{% raw %}Node under memory pressure: {{ $labels.node }}{% endraw %}"
        description: "Check RAM usage and cgroup limits"

  # -------------------------------
  # Node filesystem usage alerts
  # -------------------------------
  - name: node-filesystem-usage
    rules:
    - alert: NodeFilesystemUsageHigh
      expr: |
        (1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|overlay|squashfs|ramfs|nsfs|proc|sysfs|cgroup2?|fuse.*"}
              / node_filesystem_size_bytes{fstype!~"tmpfs|overlay|squashfs|ramfs|nsfs|proc|sysfs|cgroup2?|fuse.*"})) > 0.80
      for: 15m
      labels: { severity: "warning" }
      annotations:
        summary: "{% raw %}High FS usage (≥80%) on {{ $labels.instance }}:{{ $labels.mountpoint }}{% endraw %}"
        description: "{% raw %}Check free space / log rotation / cleanup. mount={{ $labels.mountpoint }}{% endraw %}"

    - alert: NodeFilesystemUsageCritical
      expr: |
        (1 - (node_filesystem_avail_bytes{fstype!~"tmpfs|overlay|squashfs|ramfs|nsfs|proc|sysfs|cgroup2?|fuse.*"}
              / node_filesystem_size_bytes{fstype!~"tmpfs|overlay|squashfs|ramfs|nsfs|proc|sysfs|cgroup2?|fuse.*"})) > 0.90
      for: 5m
      labels: { severity: "critical" }
      annotations:
        summary: "{% raw %}Critical FS usage (≥90%) on {{ $labels.instance }}:{{ $labels.mountpoint }}{% endraw %}"
        description: "{% raw %}Risk of running out of space. mount={{ $labels.mountpoint }}{% endraw %}"

    - alert: NodeFilesystemInodesLow
      expr: |
        (node_filesystem_files_free{fstype!~"tmpfs|overlay|squashfs|ramfs|nsfs|proc|sysfs|cgroup2?|fuse.*"}
         / node_filesystem_files{fstype!~"tmpfs|overlay|squashfs|ramfs|nsfs|proc|sysfs|cgroup2?|fuse.*"}) < 0.10
      for: 15m
      labels: { severity: "warning" }
      annotations:
        summary: "{% raw %}Low inode reserve (≤10%) on {{ $labels.instance }}:{{ $labels.mountpoint }}{% endraw %}"
        description: "{% raw %}Clean up small files. mount={{ $labels.mountpoint }}{% endraw %}"

  # -------------------------------
  # PVC usage alerts
  # -------------------------------
  - name: pvc-usage
    rules:
    - alert: PVCStorageUsageHigh
      expr: |
        (1 - (kubelet_volume_stats_available_bytes
              / kubelet_volume_stats_capacity_bytes)) > 0.85
      for: 15m
      labels: { severity: "warning" }
      annotations:
        summary: "{% raw %}PVC ≥85% used ({{ $labels.persistentvolumeclaim }}){% endraw %}"
        description: "{% raw %}Namespace={{ $labels.namespace }} PVC={{ $labels.persistentvolumeclaim }}{% endraw %}"

    - alert: PVCStorageUsageCritical
      expr: |
        (1 - (kubelet_volume_stats_available_bytes
              / kubelet_volume_stats_capacity_bytes)) > 0.95
      for: 5m
      labels: { severity: "critical" }
      annotations:
        summary: "{% raw %}PVC ≥95% used ({{ $labels.persistentvolumeclaim }}){% endraw %}"
        description: "{% raw %}Namespace={{ $labels.namespace }} PVC={{ $labels.persistentvolumeclaim }}{% endraw %}"

  # -------------------------------
  # Extra critical K8s alerts
  # -------------------------------
  - name: k8s-critical-extra
    rules:
    - alert: K8sNodeNotReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 5m
      labels: { severity: "critical" }
      annotations:
        summary: "{% raw %}Node NotReady: {{ $labels.node }}{% endraw %}"
        description: "Node unavailable for ≥5m"

  # -------------------------------
  # Application SLO (latency & errors)
  # -------------------------------
   # -------------------------------
  # Application latency recordings (DEV)
  # -------------------------------
  - name: devistor-latency-dev
    rules:
    # Fast-ratio: share of requests <= 0.5s
    - record: devistor:flask_latency_fast_ratio
      expr: |
        sum(rate(flask_http_request_duration_seconds_bucket{
          namespace="dev", path!~"/metrics|/health(|z)?", le="0.5"
        }[5m]))
        /
        clamp_min(
          sum(rate(flask_http_request_duration_seconds_count{
            namespace="dev", path!~"/metrics|/health(|z)?"
          }[5m])),
          1
        )
      labels: { env: dev }

    # ERROR RATIO (fixed): compute directly (no duplicate labelsets), safe with clamp_min
    - record: devistor:flask_latency_error_ratio_5m
      expr: |
        1 -
        (
          sum(rate(flask_http_request_duration_seconds_bucket{
            namespace="dev", path!~"/metrics|/health(|z)?", le="0.5"
          }[5m]))
          /
          clamp_min(
            sum(rate(flask_http_request_duration_seconds_count{
              namespace="dev", path!~"/metrics|/health(|z)?"
            }[5m])),
            1
          )
        )
      labels: { env: dev }

  # -------------------------------
  # Application latency recordings (PROD)
  # -------------------------------
  - name: devistor-latency-prod
    rules:
    # Fast-ratio: share of requests <= 0.5s
    - record: devistor:flask_latency_fast_ratio
      expr: |
        sum(rate(flask_http_request_duration_seconds_bucket{
          namespace="prod", path!~"/metrics|/health(|z)?", le="0.5"
        }[5m]))
        /
        clamp_min(
          sum(rate(flask_http_request_duration_seconds_count{
            namespace="prod", path!~"/metrics|/health(|z)?"
          }[5m])),
          1
        )
      labels: { env: prod }

    # ERROR RATIO (fixed): compute directly (no duplicate labelsets), safe with clamp_min
    - record: devistor:flask_latency_error_ratio_5m
      expr: |
        1 -
        (
          sum(rate(flask_http_request_duration_seconds_bucket{
            namespace="prod", path!~"/metrics|/health(|z)?", le="0.5"
          }[5m]))
          /
          clamp_min(
            sum(rate(flask_http_request_duration_seconds_count{
              namespace="prod", path!~"/metrics|/health(|z)?"
            }[5m])),
            1
          )
        )
      labels: { env: prod }

  - name: app-slo
    rules:
    - alert: API5xxRateHigh
      expr: |
        rate(http_requests_total{job=~"devistor.*",namespace=~"devistor-(prod|dev)",status=~"5.."}[5m])
        /
        rate(http_requests_total{job=~"devistor.*",namespace=~"devistor-(prod|dev)"}[5m]) > 0.05
      for: 10m
      labels: { severity: "critical" }
      annotations:
        summary: "5xx errors > 5% in API (prod/dev)"
        description: "Check dependencies/upstreams and rollouts"

    - alert: APIP95LatencyApproachingSLO
      expr: |
        (
          1000 *
          histogram_quantile(
            0.95,
            sum by (le) (
              rate(http_request_duration_seconds_bucket{job=~"devistor.*",namespace=~"devistor-(prod|dev)",route!~"/(metrics|health)"}[5m])
            )
          ) > 240
        )
        and
        sum(rate(http_request_duration_seconds_count{job=~"devistor.*",namespace=~"devistor-(prod|dev)",route!~"/(metrics|health)"}[5m])) > 0.2
      for: 10m
      labels: { severity: "warning" }
      annotations:
        summary: "p95 latency is close to SLO"
        description: "{% raw %}p95={{ printf \"%.0f\" $value }} ms (>240 ms), only with real traffic{% endraw %}"

    - alert: APIP95LatencyOverSLO
      expr: |
        (
          1000 *
          histogram_quantile(
            0.95,
            sum by (le) (
              rate(http_request_duration_seconds_bucket{job=~"devistor.*",namespace=~"devistor-(prod|dev)",route!~"/(metrics|health)"}[5m])
            )
          ) > 300
        )
        and
        sum(rate(http_request_duration_seconds_count{job=~"devistor.*",namespace=~"devistor-(prod|dev)",route!~"/(metrics|health)"}[5m])) > 0.2
      for: 15m
      labels: { severity: "critical" }
      annotations:
        summary: "p95 latency exceeds SLO 300ms"
        description: "{% raw %}p95={{ printf \"%.0f\" $value }} ms (>300 ms), with traffic >0.2 RPS{% endraw %}"

  # -------------------------------
  # Application SLO burn rate alerts
  # -------------------------------
  - name: app-slo-burn-alerts
    rules:
    - alert: DevistorLatencySLOFastBurn
      expr: slo:devistor:latency:error_budget_burn_1h > 14
            and slo:devistor:latency:error_budget_burn_6h > 14
      for: 10m
      labels:
        severity: critical
        slo: latency
        team: devistor
      annotations:
        summary: "Fast SLO error budget burn (latency)"
        description: "Burn rate >14 in 1h and 6h — risk of budget exhaustion < 1 day"

    - alert: DevistorLatencySLOSlowBurn
      expr: slo:devistor:latency:error_budget_burn_6h > 1
      for: 2h
      labels:
        severity: warning
        slo: latency
        team: devistor
      annotations:
        summary: "Slow SLO error budget burn (latency)"
        description: "Burn rate >1 in 6h — plan corrective actions"

  # -------------------------------
  # Infrastructure: Proxmox hosts
  # -------------------------------
  - name: infra-proxmox
    rules:
    - alert: NodeMemoryUsageHigh
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 0.90
      for: 10m
      labels: { severity: "warning" }
      annotations:
        summary: "{% raw %}RAM > 90% on {{ $labels.instance }}{% endraw %}"
        description: "Check processes and cgroup limits"

    - alert: NodeHighTemperatureHwmon
      expr: max without (sensor, chip) (node_hwmon_temp_celsius) > 70
      for: 10m
      labels: { severity: "warning" }
      annotations:
        summary: "{% raw %}High temperature >70°C on {{ $labels.instance }} (hwmon){% endraw %}"
        description: "Verify cooling/fans/dust"

    - alert: NodeHighTemperatureThermal
      expr: max by (instance) (node_thermal_zone_temp / 1000) > 70
      for: 10m
      labels: { severity: "warning" }
      annotations:
        summary: "{% raw %}High temperature >70°C on {{ $labels.instance }} (thermal_zone){% endraw %}"
        description: "Verify cooling/fans/dust"

    - alert: ProxmoxHostDown
      expr: up{job=~"node-exporter|pve|prometheus-pve-exporter", node=~"proxmox-.*"} == 0
      for: 2m
      labels: { severity: "critical" }
      annotations:
        summary: "{% raw %}Proxmox host down: {{ $labels.instance }}{% endraw %}"
        description: "No metrics scraped from host"

  # -------------------------------
  # Backup monitoring: Velero, PBS, DB dumps
  # -------------------------------
  - name: backups-and-velero
    rules:
    - alert: VeleroBackupFailedRecently
      expr: sum(increase(velero_backup_attempt_total{phase=~"Failed|PartiallyFailed"}[15m])) > 0
      for: 5m
      labels: { severity: "critical" }
      annotations:
        summary: "{% raw %}Velero: new failed backup{% endraw %}"
        description: "Check Velero logs and storage"

    - alert: VeleroBackupStale24h
      expr: (time() - max by (schedule) (velero_backup_last_successful_timestamp)) > 24*60*60
      for: 30m
      labels: { severity: "warning" }
      annotations:
        summary: "{% raw %}Velero: no successful backup >24h (schedule={{ $labels.schedule }}){% endraw %}"
        description: "Verify job and object storage access"

    - alert: PBSVmBackupStale24h
      expr: (time() - max by (vm_id, vm_name) (pbs_snapshot_vm_last_timestamp)) > 24*60*60
      for: 30m
      labels: { severity: "warning" }
      annotations:
        summary: "{% raw %}PBS: stale VM backup >24h ({{ $labels.vm_name }} / {{ $labels.vm_id }}){% endraw %}"
        description: "Check schedule and datastore"

    - alert: DatabaseDumpStale24h
      expr: (time() - max(vip_backup_db_last_success_unixtime)) > 24*60*60
      for: 30m
      labels: { severity: "warning" }
      annotations:
        summary: "{% raw %}DB dump: no success >24h{% endraw %}"
        description: "Verify CronJob/Pushgateway/connectivity"

  # -------------------------------
  # Prometheus internal alerts
  # -------------------------------
  - name: monitoring-prometheus
    rules:
    - alert: PrometheusRuleFailures
      expr: increase(prometheus_rule_evaluation_failures_total[5m]) > 0
      for: 10m
      labels: { severity: "warning" }
      annotations:
        summary: "Prometheus: rule evaluation errors (last 5m)"
        description: "At least one rule evaluation failed. Check Prometheus logs and rule definitions (e.g. duplicate labelsets)."
